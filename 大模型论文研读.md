# 大模型
## 偏好学习
### [ORPO:  Monolithic Preference Optimization without Reference Model](https://arxiv.org/abs/2403.07691)
- 相比于RLHF 不需要 偏好模型；
- 通过赔率比（odds ratio）为优劣的答案增加奖励或者惩罚；

### DPO
- policy model: 策略模型，需要微调的模型，也是训练完DPO后得到的模型；
- reference model: 参考模型，不需要微调的模型，也就是原始模型，用于和策略模型对比在同一答案下的生成概率；
  
- beta 指隐式奖励的超参数，beta是DPO损失的温度参数，通常取值范围在0.1到0.5之间。当beta趋近于0时，我们可以忽略参考模型。

- 指标
  - 在DPO（Direct Preference Optimization）训练和评估过程中，以下几个奖励指标的取值范围和意义如下：

  - rewards/chosen：
    - 取值范围：这个指标表示策略模型与参考模型在“chosen”响应上的对数概率差的平均值，经过beta参数的缩放。通常，这个值可以为正或负，具体取决于策略模型在优化过程中如何调整相对于参考模型的生成概率。
    - 意义：正值表示策略模型倾向于比参考模型更好地生成符合人类偏好的回答；负值则表示策略模型在这一方面表现较差。
  - rewards/rejected：
    - 取值范围：类似于rewards/chosen，这个指标表示策略模型与参考模型在“rejected”响应上的对数概率差的平均值，经过beta缩放。这个值也可能为正或负。
    - 意义：正值表示策略模型在生成较差回答时比参考模型更可能避开这些回答；负值表示策略模型在这个方面可能需要进一步优化。
  - rewards/accuracies：
    - 取值范围：这个指标表示“chosen”奖励高于“rejected”奖励的频率的平均值，通常介于0和1之间。
    - 意义：这个值越接近1，表示策略模型越频繁地正确选择了人类偏好的回答。这是衡量模型是否在优化过程中准确区分“好”与“坏”回答的关键指标。
  - rewards/margins：
    - 取值范围：表示“chosen”响应和对应“rejected”响应之间的奖励差异的平均值。这个值可以为正或负，具体取决于策略模型的输出质量。
    - 意义：正值意味着策略模型的“chosen”响应明显优于“rejected”响应，模型对不同回答之间的差异更为敏感。负值则表明需要更多的优化。
- 经验：
  - beta从0.05到0.5之间的调整还是很有必要实验的，一般只推荐训练 1 个epoch即可
